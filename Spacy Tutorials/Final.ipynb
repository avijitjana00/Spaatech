{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84919a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4eca7c65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pypdf2"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -heel (c:\\users\\avi\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -heel (c:\\users\\avi\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -heel (c:\\users\\avi\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -heel (c:\\users\\avi\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -heel (c:\\users\\avi\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -heel (c:\\users\\avi\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -heel (c:\\users\\avi\\anaconda3\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Downloading PyPDF2-1.27.6-py3-none-any.whl (68 kB)\n",
      "Installing collected packages: pypdf2\n",
      "Successfully installed pypdf2-1.27.6\n"
     ]
    }
   ],
   "source": [
    "#!pip install docx2txt\n",
    "!pip install pypdf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2c95c18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import docx2txt\n",
    "from PyPDF2 import PdfFileReader, PdfFileWriter, PdfFileMerger\n",
    "\n",
    "#Extracting text from DOCX\n",
    "def doctotext(m):\n",
    "    temp = docx2txt.process(m)\n",
    "    resume_text = [line.replace('\\t', ' ') for line in temp.split('\\n') if line]\n",
    "    text = ' '.join(resume_text)\n",
    "    return (text)\n",
    "\n",
    "\n",
    "## Extracting text from PDF\n",
    "def pdftotext(m):\n",
    "    # pdf file object\n",
    "    # you can find find the pdf file with complete code in below\n",
    "    pdfFileObj = open(m, 'rb')\n",
    "\n",
    "    # pdf reader object\n",
    "    pdfFileReader = PdfFileReader(pdfFileObj)\n",
    "\n",
    "    # number of pages in pdf\n",
    "    num_pages = pdfFileReader.numPages\n",
    "\n",
    "    currentPageNumber = 0\n",
    "    text = ''\n",
    "\n",
    "    # Loop in all the pdf pages.\n",
    "    while(currentPageNumber < num_pages ):\n",
    "\n",
    "        # Get the specified pdf page object.\n",
    "        pdfPage = pdfFileReader.getPage(currentPageNumber)\n",
    "\n",
    "        # Get pdf page text.\n",
    "        text = text + pdfPage.extractText()\n",
    "\n",
    "        # Process next page.\n",
    "        currentPageNumber += 1\n",
    "    return (text)\n",
    "  \n",
    "if __name__ == '__main__': \n",
    "\n",
    "    FilePath = 'DS_CV.pdf'\n",
    "    FilePath.lower().endswith(('.png', '.docx'))\n",
    "    if FilePath.endswith('.docx'):\n",
    "        textinput = doctotext(FilePath) \n",
    "    elif FilePath.endswith('.pdf'):\n",
    "        textinput = pdftotext(FilePath)\n",
    "    else:\n",
    "        print(\"File not support\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "67831604",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Avi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Avi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Avi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\Avi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\Avi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "from spacy.matcher import Matcher\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5f9ca7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e5eba60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher = Matcher(nlp.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fac741fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name:  AV IJ\n"
     ]
    }
   ],
   "source": [
    "def extract_name(resume_text):\n",
    "    nlp_text = nlp(resume_text)\n",
    "    \n",
    "    # First name and Last name are always Proper Nouns\n",
    "    pattern = [{'POS': 'PROPN'},{'POS': 'PROPN'}]\n",
    "    \n",
    "    matcher.add('NAME',[pattern])\n",
    "    \n",
    "    matches = matcher(nlp_text)\n",
    "    \n",
    "    for match_id, start, end in matches:\n",
    "        span = nlp_text[start:end]\n",
    "        return span.text\n",
    "print('Name: ',extract_name(textinput))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c7f1f175",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a0e6ceb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "STOPWORDS = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "45f22e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#STOPWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "664faecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qualification:  ['x', 'MS']\n"
     ]
    }
   ],
   "source": [
    "EDUCATION = [\n",
    "            'BE','B.E.', 'B.E', 'BS', 'B.S', \n",
    "            'ME', 'M.E', 'M.E.', 'M.B.A', 'MBA', 'MS', 'MSC', 'IN IT(DATA SCIENCE)',\n",
    "            'BTECH', 'B.TECH', 'M.TECH', 'MTECH', \n",
    "            'SSLC', 'SSC' 'HSC', 'CBSE', 'ICSE', 'X', 'XII','HIGHER SECONDARY','B.SC(H)','(COMPUTER SCIENCE)'\n",
    "        ]\n",
    "def extract_education(resume_text):\n",
    "    nlp_text = nlp(resume_text)\n",
    "    # Sentence Tokenizer\n",
    "    nlp_text = [sent.string.strip() for sent in nlp_text.sents]\n",
    "\n",
    "    edu = {}\n",
    "    # Extract education degree\n",
    "    for index, text in enumerate(nlp_text):\n",
    "        for tex in text.split():\n",
    "            # Replace all special symbols\n",
    "            tex = re.sub(r'[?|$|.|!|,]', r'', tex)\n",
    "            if tex.upper() in EDUCATION and tex not in STOPWORDS:\n",
    "                edu[tex] = text + nlp_text[index + 1]\n",
    "def extract_education(resume_text):\n",
    "    nlp_text = nlp(resume_text)\n",
    "\n",
    "    # Sentence Tokenizer\n",
    "    nlp_text = [sent.text.strip() for sent in nlp_text.sents]\n",
    "    edu = {}\n",
    "    # Extract education degree\n",
    "    for index, text in enumerate(nlp_text):\n",
    "        for tex in text.split():\n",
    "            # Replace all special symbols\n",
    "            tex = re.sub(r'[?|$|.|!|,]', r'', tex)\n",
    "            if tex.upper() in EDUCATION and tex not in STOPWORDS:\n",
    "                edu[tex] = text + nlp_text[index + 1]\n",
    "                \n",
    "                \n",
    "\n",
    "    # Extract year\n",
    "    education = []\n",
    "    for key in edu.keys():\n",
    "        year = re.search(re.compile(r'(((20|19)(\\d{})))'), edu[key])\n",
    "        if year:\n",
    "            education.append((key, ''.join(year[0])))\n",
    "        else:\n",
    "            education.append(key)\n",
    "    return education\n",
    "print('Qualification: ',extract_education(textinput))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "66d70928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mail id:  []\n"
     ]
    }
   ],
   "source": [
    "def extract_email_addresses(string):\n",
    "    r = re.compile(r'[\\w\\.-]+@[\\w\\.-]+')\n",
    "    return r.findall(string)\n",
    "print('Mail id: ',extract_email_addresses(textinput))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0693fba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mobile Number:  +9 195478 50091                                                                                    \n"
     ]
    }
   ],
   "source": [
    "def extract_mobile_number(resume_text):\n",
    "    phone = re.findall(re.compile(r'([+(]?\\d+[)\\-]?[ \\t\\r\\f\\v]*[(]?\\d{2,}[()\\-]?[ \\t\\f\\f\\v]*\\d{2,}[()\\-]?[\\t\\r\\f\\v]*\\d*[ \\t\\r\\f\\v]*)'), resume_text)\n",
    "    \n",
    "    if phone:\n",
    "        number = ''.join(phone[0])\n",
    "        if len(number) > 10:\n",
    "            return number\n",
    "        else:\n",
    "            return number\n",
    "print('Mobile Number: ',extract_mobile_number(textinput))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df5a496",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c69ec34a",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'English' object has no attribute 'noun_chunks'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9776/3533735953.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# load pre-trained model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mnlp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'en_core_web_sm'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mnoun_chunks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnoun_chunks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mextract_skills\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresume_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'English' object has no attribute 'noun_chunks'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "\n",
    "# load pre-trained model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "noun_chunks = nlp.noun_chunks\n",
    "\n",
    "def extract_skills(resume_text):\n",
    "    nlp_text = nlp(resume_text)\n",
    "\n",
    "    # removing stop words and implementing word tokenization\n",
    "    tokens = [token.text for token in nlp_text if not token.is_stop]\n",
    "    \n",
    "    # reading the csv file\n",
    "    data = pd.read_csv(\"skills.csv\") \n",
    "    \n",
    "    # extract values\n",
    "    skills = list(data.columns.values)\n",
    "    \n",
    "    skillset = []\n",
    "    \n",
    "    # check for one-grams (example: python)\n",
    "    for token in tokens:\n",
    "        if token.lower() in skills:\n",
    "            skillset.append(token)\n",
    "    \n",
    "    # check for bi-grams and tri-grams (example: machine learning)\n",
    "    for token in noun_chunks:\n",
    "        token = token.text.lower().strip()\n",
    "        if token in skills:\n",
    "            skillset.append(token)\n",
    "    \n",
    "    return [i.capitalize() for i in set([i.lower() for i in skillset])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71dca63",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
